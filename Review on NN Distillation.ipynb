{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble: a collection of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft targets: the class probabilities, such as the softmax probabilities, produced by a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transfer set: a subset of the training set that is used as the new training set to transfer an ensemble to the finalist model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives aiming to achieve in the finalist model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encourage the finalist model to predict the true targets as well as matching the soft targets provided by the ensemble, without too much computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of distillation is actually closely related to statistical mechanics. The logit $z_i$ refers to the energy at state $i$ from an ensemble.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Gibbs'distribution law, $p_i =\\frac{\\exp(\\beta{z_i})}{\\sum_{j} \\exp(\\beta{z_j})}$ is the probability of the energy state at $i$,  where $\\beta = -\\frac{1}{\\kappa T}$ and $\\kappa$ is the Boltzmann constant, $T$ is the temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, $\\kappa = -1$ is set. If $T = 1$, this is the typical softmax probability $p_i =\\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)} $ we see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy state $z_i$ here refers to the $i-$th output from the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the higher $T$, the distribution is softer in the sense that $p(z)$ have less fluctuation. When $T \\rightarrow \\infty$, $p(z)$ tends to be a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distilled model is trained on a transfer set and use a soft target distribution produced by using higher $T$ in all $p_i$. After training, we set $T=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ensembles of very big datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem arised: excessive computation with (1) huge neural network for each model, and (2) large dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: learn specialist models that each concentrates on a different confusable subset of the classes, but should avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Find the confused classes (different kinds of mushroom) that are specifically trained on a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) combine all of the classes it does not care (e.g. remaining classes) about into a single dustbin class on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) The proportion of the training examples from the confused classes shall be higher in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Specified in the paper: half examples from confused classes and another half from the single dustbin class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) the model is initialized with the weights of the generalist model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) After training, we can make correction on the biased training\n",
    "set by incrementing the logit of the dustbin class by the log of the proportion by which the\n",
    "specialist class is oversampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43632898, -0.4978327 ,  1.67371774,  0.29129533,  0.57995694],\n",
       "       [-0.78937289, -0.85106126, -0.82978901,  0.894575  ,  1.0703239 ],\n",
       "       [-0.49457959, -0.42699805,  0.93182444,  1.43433881,  1.51625282]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Assume there are 5 training examples.\n",
    "Let's say, the 1st and 2nd rows of the logit z come from the confused classes; \n",
    "the 3rd row comes from the single dustbin class.\n",
    "'''\n",
    "z = np.random.randn(3,5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3919959093788017, 0.10800409062119831, 0.5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Assume p is the proportion array by which the specialist class is oversampled.\n",
    "pc1 refers to the proportion from the confused class 1.\n",
    "'''\n",
    "pc1 = np.random.uniform(low = 0, high = 0.5)\n",
    "p = [pc1, 0.5-pc1, 0.5]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43632898, -0.4978327 ,  1.67371774,  0.29129533,  0.57995694],\n",
       "       [-0.78937289, -0.85106126, -0.82978901,  0.894575  ,  1.0703239 ],\n",
       "       [-1.18772677, -1.12014523,  0.23867726,  0.74119163,  0.82310564]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[-1,:] += np.log(p[-1])\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details in Training ensembles of very big datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Find the confused classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpler approach that does not require the true labels to construct the clusters is better. Thus, we avoid using confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, apply a clustering algorithm to the covariance matrix of the predictions of our\n",
    "generalist model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing inference with ensembles of specialists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given probability distributions $\\mathbb{P},\\mathbb{Q}$, we have the probability densities $p = \\frac{d\\mathbb{P}}{dx}, q = \\frac{d\\mathbb{Q}}{dx}$ defined under certain assumption (absolute continuity of distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$KL(p || q) = \\int \\log(\\frac{d\\mathbb{P}}{d\\mathbb{Q}}) \\mathrm{d}\\mathbb{P} = \\int\\log(\\frac{p}{q})p \\mathrm{d} x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method measuring the distance of two distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided the density $q$, and sample $x$ over $p$, $KL(p || q)$ is the expectation of log-difference between $p$ and $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$KL(p, q1) + KL(p, q2) + KL(p, q3) = \\int \\big(\\log(\\frac{p}{q1}) + \\log(\\frac{p}{q2}) + \\log(\\frac{p}{q3})  \\big) p \\mathrm{d} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $n$ most classes $C$ that are probable according to the generalist model. In the paper, $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the specialist models, $m$, with the confusable classes $S^m$. Put $A_k = C\\cap S^m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $q, p^m, p^g$ are the probability density of full classes, classes from specialist model $m$ and classes $C$ of generalist model respectively such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$KL(p^g, q) + \\sum_{m \\in A_{k}}KL(p^m, q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In detail, $q = softmax(z)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
